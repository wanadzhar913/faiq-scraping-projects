{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gc\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Scraping **The Edge Malaysia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [The Edge Malaysia](https://theedgemalaysia.com/), each of their articles seem to have a unique ID, e.g., \"https://theedgemalaysia.com/node/677590\". Hence, since we won't be able to do this by month, page no., etc., we'll use a **brute force** approach that tests every combination of numbers, such that we'll only scrape from a valid url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Referer': 'https://theedgemalaysia.com/node/670007',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"116\", \"Not)A;Brand\";v=\"24\", \"Google Chrome\";v=\"116\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text and metadata from url\n",
    "def process_url(x):\n",
    "    while True:\n",
    "        webpage = f'https://theedgemalaysia.com/node/{x}'\n",
    "        try:\n",
    "            r = requests.get(webpage, headers=headers)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(5.0)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('title').text\n",
    "        h = soup.find('div', class_=\"news-detail_newsTextDataWrap__PkAu5\") \n",
    "        content_list = [p.text for p in h.find_all(\"p\")]\n",
    "        content_str = ' '.join(content_list)\n",
    "        if 'English version' in content_str:\n",
    "            language = 'Mandarin'\n",
    "        else:\n",
    "            language = 'English'\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    data = {'url': f'https://theedgemalaysia.com/node/{x}', \n",
    "            'headline': headline,\n",
    "            'language': language,\n",
    "            'content': content_str}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initially I only wanted to scrape a subset of the website.\n",
    "I decided to proceed with the rest after batch 1 - 7.\n",
    "\n",
    "Pertaining to below, the range I initially specified neglects numbers like\n",
    "200000, 300000, 400000, etc. I've fixed this batch8 onwards.\n",
    "\n",
    "NOTE: np.random.randint produces duplicates!\n",
    "\"\"\"\n",
    "np.random.seed(10082023)\n",
    "batch1 = list(set([str(x) for x in np.random.randint(0, 100000, size=50000)]))\n",
    "batch2 = list(set([str(x) for x in np.random.randint(100000, 200000, size=50000)]))\n",
    "batch3 = list(set([str(x) for x in np.random.randint(200000, 300000, size=50000)]))\n",
    "batch4 = list(set([str(x) for x in np.random.randint(300000, 400000, size=50000)]))\n",
    "batch5 = list(set([str(x) for x in np.random.randint(400000, 500000, size=50000)]))\n",
    "batch6 = list(set([str(x) for x in np.random.randint(500000, 600000, size=50000)]))\n",
    "batch7 = list(set([str(x) for x in np.random.randint(600000, 700001, size=50000)]))\n",
    "\n",
    "batch8 = list(\n",
    "    set([str(x) for x in np.arange(500000, 600000)]) - set(batch6)\n",
    ")\n",
    "\n",
    "batch9 = list(\n",
    "    set([str(x) for x in np.arange(400000, 500000)]) - set(batch5)\n",
    ")\n",
    "\n",
    "batch10 = list(\n",
    "    set([str(x) for x in np.arange(300000, 400000)]) - set(batch4)\n",
    ")\n",
    "\n",
    "batch11 = list(\n",
    "    set([str(x) for x in np.arange(200000, 300000)]) - set(batch3)\n",
    ")\n",
    "\n",
    "batch12 = list(\n",
    "    set([str(x) for x in np.arange(100000, 200000)]) - set(batch2)\n",
    ")\n",
    "\n",
    "batch13 = list(\n",
    "    set([str(x) for x in np.arange(0, 100000)]) - set(batch1)\n",
    ")\n",
    "\n",
    "batch14 = list(\n",
    "    set([str(x) for x in np.arange(600000, 700001)]) - set(batch7)\n",
    ")\n",
    "\n",
    "batches = [batch1, batch2, batch3, batch4, batch5, \n",
    "           batch6, batch7, batch8, batch9, batch10, \n",
    "           batch11, batch12, batch13, batch14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't set it to this much if you're scraping locally! Use a lower max_worker.\n",
    "max_worker = 100\n",
    "\n",
    "for j, urls in enumerate(batches):\n",
    "    for i in tqdm(range(0, len(urls), max_worker)):\n",
    "        gc.collect()\n",
    "        with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "            futures = {executor.submit(process_url, t): t for t in urls[i: i + max_worker]}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                with open(f'theedgemalaysia--complete-batch-{j+1}.jsonl', 'a') as final:\n",
    "                    json.dump(result, final)\n",
    "                    final.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_check = [\n",
    "    'theedgemalaysia--complete-batch-1.jsonl',\n",
    "    'theedgemalaysia--complete-batch-2.jsonl',\n",
    "    'theedgemalaysia--complete-batch-3.jsonl',\n",
    "    'theedgemalaysia--complete-batch-4.jsonl',\n",
    "    'theedgemalaysia--complete-batch-5.jsonl',\n",
    "    'theedgemalaysia--complete-batch-6.jsonl',\n",
    "    'theedgemalaysia--complete-batch-7.jsonl',\n",
    "    'theedgemalaysia--complete-batch-8.jsonl',\n",
    "    'theedgemalaysia--complete-batch-9.jsonl',\n",
    "    'theedgemalaysia--complete-batch-10.jsonl',\n",
    "    'theedgemalaysia--complete-batch-11.jsonl',\n",
    "    'theedgemalaysia--complete-batch-12.jsonl',\n",
    "    'theedgemalaysia--complete-batch-13.jsonl',\n",
    "    'theedgemalaysia--complete-batch-14.jsonl',\n",
    "    ]\n",
    "\n",
    "dfs = []\n",
    "for i, file in enumerate(files_to_check):\n",
    "    get_df = pd.read_json(file, lines=True)\n",
    "    get_df.drop_duplicates(inplace=True)\n",
    "    dfs.append(get_df)\n",
    "    print(f'No. of articles in batch {i+0}: {len(get_df)}')\n",
    "\n",
    "final_num_articles = pd.concat(dfs, axis=0)\n",
    "final_num_articles.to_parquet('theedgemalaysia--complete-1-14.parquet', index=False)\n",
    "print(f'Num. of articles/webpages scraped: {len(final_num_articles)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
