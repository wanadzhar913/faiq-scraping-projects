{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gc\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list down the sections of the website we'd like to scrape.\n",
    "sections = [\n",
    "    \"cars/\",\n",
    "    \"fashion/\",\n",
    "    \"lifestyle/hotels/\",\n",
    "    \"lifestyle/watches/\",\n",
    "    \"travel/\",\n",
    "    \"technology-gadgets/\",\n",
    "    \"dining/premium-dining/\",\n",
    "    \"dining/romantic-dining-spots/\",\n",
    "    \"media/music-concerts/\",\n",
    "    \"random/news/\",\n",
    "    \"finance/\",\n",
    "    \"retail/\",\n",
    "    \"random/editors-note/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookies = {\n",
    "    '__utma': '129007881.1867875053.1694272480.1694272480.1694272480.1',\n",
    "    '__utmc': '129007881',\n",
    "    '__utmz': '129007881.1694272480.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none)',\n",
    "    '__ATA_tuuid': '6288fef3-89f2-4108-aa81-3d96a6b17778',\n",
    "    '__utmt': '1',\n",
    "    '__utmb': '129007881.47.9.1694273467830',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'authority': 'timchew.net',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': '__utma=129007881.1867875053.1694272480.1694272480.1694272480.1; __utmc=129007881; __utmz=129007881.1694272480.1.1.utmcsr=(direct)|utmccn=(direct)|utmcmd=(none); __ATA_tuuid=6288fef3-89f2-4108-aa81-3d96a6b17778; __utmt=1; __utmb=129007881.47.9.1694273467830',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"116\", \"Not)A;Brand\";v=\"24\", \"Google Chrome\";v=\"116\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'none',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Get number of pages in each section, and their hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_pagenums(url):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, cookies=cookies, headers=headers)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    block = soup.find('div',attrs = {\"class\":\"site-content cf\"})\n",
    "    \n",
    "    if block is None:\n",
    "        return\n",
    "    hrefs = []\n",
    "    for link in block.find_all('h2', attrs = {\"class\":\"entry-title\"}):\n",
    "        try:\n",
    "            href = link.find('a').get('href')\n",
    "            hrefs.append(href)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting pages from section: \"cars/\"\n",
      "'Cars/' has 6 unique pages.\n",
      "\n",
      "Getting pages from section: \"fashion/\"\n",
      "'Fashion/' has 6 unique pages.\n",
      "\n",
      "Getting pages from section: \"lifestyle/hotels/\"\n",
      "'Lifestyle/Hotels/' has 2 unique pages.\n",
      "\n",
      "Getting pages from section: \"lifestyle/watches/\"\n",
      "'Lifestyle/Watches/' has 3 unique pages.\n",
      "\n",
      "Getting pages from section: \"travel/\"\n",
      "'Travel/' has 4 unique pages.\n",
      "\n",
      "Getting pages from section: \"technology-gadgets/\"\n",
      "'Technology-Gadgets/' has 7 unique pages.\n",
      "\n",
      "Getting pages from section: \"dining/premium-dining/\"\n",
      "'Dining/Premium-Dining/' has 3 unique pages.\n",
      "\n",
      "Getting pages from section: \"dining/romantic-dining-spots/\"\n",
      "'Dining/Romantic-Dining-Spots/' has 3 unique pages.\n",
      "\n",
      "Getting pages from section: \"media/music-concerts/\"\n",
      "'Media/Music-Concerts/' has 2 unique pages.\n",
      "\n",
      "Getting pages from section: \"random/news/\"\n",
      "'Random/News/' has 2 unique pages.\n",
      "\n",
      "Getting pages from section: \"finance/\"\n",
      "'Finance/' has 3 unique pages.\n",
      "\n",
      "Getting pages from section: \"retail/\"\n",
      "'Retail/' has 2 unique pages.\n",
      "\n",
      "Getting pages from section: \"random/editors-note/\"\n",
      "'Random/Editors-Note/' has 3 unique pages.\n",
      "\n",
      "Num. of unique page numbers: 33\n",
      "Num. of unique articles: 839\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "hrefs = []\n",
    "for section in sections:\n",
    "    print(f'Getting pages from section: \"{section}\"')\n",
    "    i = 1\n",
    "    while section is not None:\n",
    "        url = f\"https://timchew.net/category/{section}page/{i}/\"\n",
    "        response = requests.get(url, cookies=cookies, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            if i > 1:\n",
    "                url_b4 = f\"https://timchew.net/category/{section}page/{i-1}/\"\n",
    "                links = crawl_pagenums(url)\n",
    "                link_b4 = crawl_pagenums(url_b4)\n",
    "                if links != link_b4:\n",
    "                    hrefs.extend(links)\n",
    "                    pages.append(url)\n",
    "                    i += 1\n",
    "                else:\n",
    "                    print(f\"'{section.title()}' has {i} unique pages.\")\n",
    "                    print(\"\")\n",
    "                    break\n",
    "            else:\n",
    "                links = crawl_pages(url)\n",
    "                hrefs.extend(links)\n",
    "                pages.append(url)\n",
    "                i += 1\n",
    "        else:\n",
    "            print(f\"'{section.title()}' has {i} unique pages.\")\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "numpages_unique = list(set(pages))\n",
    "hrefs_unique = list(set(hrefs))\n",
    "print(f'Num. of unique page numbers: {len(numpages_unique)}')\n",
    "print(f'Num. of unique articles: {len(hrefs_unique)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'timchew_numpages.json', 'a') as numpages:\n",
    "    json.dump(numpages_unique, numpages)\n",
    "with open(f'timchew_articles.json', 'a') as articles:\n",
    "    json.dump(hrefs_unique, articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Get text data from each webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of articles to collect text data from: 839\n"
     ]
    }
   ],
   "source": [
    "articles_to_scrape = []\n",
    "with open(f'timchew_articles.json') as fopen:\n",
    "    links = json.load(fopen)\n",
    "articles_to_scrape.extend(links)\n",
    "print(f\"Num. of articles to collect text data from: {len(articles_to_scrape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://timchew.net/2014/11/24/lenovo-yoga-3-pro-and-yoga-tablet-2-pro-launch-at-the-roof-first-avenue-bandar-utama/',\n",
       " 'https://timchew.net/2017/10/09/news-october-2017/',\n",
       " 'https://timchew.net/2015/12/01/gift-ideas-for-her-dior-paradise-2015-collection/',\n",
       " 'https://timchew.net/2020/10/23/kamoshibito-kuheiji-sake-pairing-dinner-by-tmi-trading-at-dc-restaurant-by-darren-chin/',\n",
       " 'https://timchew.net/2017/06/23/italian-prosecco-brunch-at-villa-danieli-sheraton-imperial-kuala-lumpur/']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_to_scrape[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_article(url):\n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, cookies=cookies, headers=headers)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('h1', class_=\"entry-title\").text\n",
    "        words = soup.find('div', attrs = {\"class\":\"entry-content\"}) # post-body entry-content\n",
    "        content = words.text\n",
    "\n",
    "    except Exception as e:\n",
    "        print('error in link:'+ url)\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    data = {'url': url, 'headline': headline, 'content': content}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e82b14270e8486293110c70ad7a4c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_worker = 50\n",
    "\n",
    "for i in tqdm(range(0, len(articles_to_scrape), max_worker)):\n",
    "    gc.collect()\n",
    "    with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "        futures = {executor.submit(crawl_article, t): t for t in articles_to_scrape[i: i + max_worker]}\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            with open(f'timchew-scraped-data.jsonl', 'a') as final:\n",
    "                json.dump(result, final)\n",
    "                final.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of rows in dataframe: 839\n",
      "Num. of webpages with no content: Empty DataFrame\n",
      "Columns: [url, headline, content]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [url, headline, content]\n",
       "Index: []"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test = pd.read_json('timchew-scraped-data.jsonl', lines=True)\n",
    "print(f\"Num. of rows in dataframe: {len(final_test)}\")\n",
    "\n",
    "# check if there's any articles with no content\n",
    "no_content = final_test[final_test['content'] == \"\"]\n",
    "print(f\"Num. of webpages with no content: {no_content}\")\n",
    "no_content.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
